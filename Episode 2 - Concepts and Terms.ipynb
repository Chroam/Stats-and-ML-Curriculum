{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f896ad0",
   "metadata": {},
   "source": [
    "Purpose: The goal of this notebook is to discuss topics surrounding hypothesis testing in frequentist statistics. This notebook will be mostly theoretical curriculum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee271b",
   "metadata": {},
   "source": [
    "<b> Introduction to Hypothesis Testing in Frequentist Statistics </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dae193",
   "metadata": {},
   "source": [
    "Hypothesis testing is how we tell if there is a meaningful difference between two or more groups. There are many tests to do this. However, the test does not prove or disprove a hypothesis. The first reason is that science does not prove anything as factual or objective; science seeks to provide evidence for a concept that must be falsifiable. See Karl Popper's \"Logic of Scientific Discovery\" for the original material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8ec09",
   "metadata": {},
   "source": [
    "The issue is now, how do we organize a hypothesis test. We need to collect data (experimental units) on variables, often in experimental groups. The names are proliferated but you will often hear dependent variables (measured, result), independent variables (manipulated, explanatory), control variables (stable), exogenous variables, confounding variables (biasing), nuisance, lurking, moderating (supressing, promoting), mediating, quantitative (discrete (interval, ratio), continuous (interval, ratio)), qualitative (nominal, ordinal), latent variables, as well as control group, manipulated group, and much more. \n",
    "\n",
    "This becomes very complex and one can spend a whole career on experimental design. The key point here is that we should organize our groups and variables in such as way that (1.) we measure what we think we are measuring and (2.) we control for error that might cause noise in our measure. \n",
    "\n",
    "As an advanced topic, we do not have to control for error in the experimental design. An alternative is to control for it statistically, with one example being a confound in an ANCOVA, however this will result in less statatistical power when compared to experimental control. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d247e2",
   "metadata": {},
   "source": [
    "Video: https://www.youtube.com/watch?v=0oc49DyA3hU&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=9\n",
    "\n",
    "\"Think of how [dumb] the average person is, and realize half of them are [dumber] than that.\" - George Carlin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e412774",
   "metadata": {},
   "source": [
    "<b> Terms of Frequentism: Significance and p-values </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d43c6",
   "metadata": {},
   "source": [
    "P-values tell us how confident we can be in our findings not being the result of random chance. If we want to be 95% confident our findings were not due to random chance, then we have a significance threshold of .05, also known as alpha. More on alpha later. If a p-value from a statistical analysis is less than .05 (if that is our preferred threshold), than we have found signficance. In everyday terms, this means there is something interesting happening in the data. If we properly controlled the data collection, then we can say with 95% confidence that whatever interesting thing happened was due to the manipulation. In other words, if we conducted this experiment 100 times, we would be wrong 5% of the time. \n",
    "\n",
    "Consider this arbitrary example. Two teams are studying for a test. Team 1 studies individually and x-bar is equal to 77. Team 2 studies in peer settings and their average is 98. We conduct a t-test to compare them (more on general linear models later) and a p-value of .0045 is found. This means that there is something interesting in the day, and we can conclude it was because of peer-learning. \n",
    "\n",
    "This can also be done with correlations and other tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4c77b",
   "metadata": {},
   "source": [
    "Video (p-values): https://www.youtube.com/watch?v=vemZtEM63GY&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=11\n",
    "        \n",
    "Video (calculate): https://www.youtube.com/watch?v=JQc3yx0-Q9E&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83704d",
   "metadata": {},
   "source": [
    "\"You need extraordinary evidence for extraordinary hypotheses\" – Laplace, 1812"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c6b9f2",
   "metadata": {},
   "source": [
    "<b> Alpha (significance threshold, type I error rate) and Beta (power, type II error rate) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef35f3",
   "metadata": {},
   "source": [
    "Remember statistics help humans reason with probabilities, we do not do this very well naturally. Some of our significance values, typically  alpha = .05, is the threshold we should use to find meaning. HOWEVER, consider the importance of what you are testing. If you are doing business analytics for some simple exploration, an alpha level of .1 might be reasonable. If you are measuring healthcare or trying to justify something bizarre, you want a very small alpha, such as .01 (1% chance of error or type 1 error and thus, alpha is the probability of a significant result when H0 is true.). \n",
    "\n",
    "beta is the probability of a nonsignificant result, given that the alternative is true, or type 2 error. 1-beta = statistical power (the probability of a significant result when alternative is true). In other words, it is how likely you are to detect a significant effect when there really is an effect. This number is not set by convention like alpha often is (more on this later, it is called the Fisher & Neyman Pearson debate). Power is computed through things like the test, amount of items in the test, specifiying 1-tail over 2-tail (at the expense of more robust findings in both tails), or most commonly by increasing the number of experimental units (the sample). \n",
    "\n",
    "Video: https://www.youtube.com/watch?v=Rsc5znwR5FA&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=14\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=bsZGt-caXO4&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=35\n",
    "\n",
    "Bonus: It is worth your time to download GPower and use the calculator to compute power for some tests.\n",
    "\n",
    "Reading: https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=VX_M3tIyiYk&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=15\n",
    "\n",
    "Reading: Type 1 Errors Inflated: Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. http://doi.org/10.1177/0956797611417632\n",
    "\n",
    "Reading: Type 1 Error Control: Rutherford, A. (2011). ANOVA and ANCOVA: a GLM approach (2nd ed). Hoboken, N.J: Wiley. Sections 3.6-3.10\n",
    "\n",
    "Reading:Introduction to Power: Cohen, J. (1992). Statistical power analysis. Current Directions in Psychological Science, 1(3), 98–101.\n",
    "\n",
    "There is also some debate over whether a post-hoc power analysis is appropriate. When designing an experiment, it is required that you determine your power, and something like GPower will give you the sample size. However, some statisticians argue that you should compute power after the data collection too, as your sample size is probably not exact.\n",
    "\n",
    "Advanced: You can also improve power through advanced methods. One example is an experimental design where the within-groups design with a within measure correlation >.5. \n",
    "\n",
    "\"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.\" H.G.Wells"
   ]
  },
  {
   "attachments": {
    "Error%20Chart.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "479ba401",
   "metadata": {},
   "source": [
    "![Error%20Chart.PNG](attachment:Error%20Chart.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10156bf5",
   "metadata": {},
   "source": [
    "A trending concept is science is the idea of Pre-Registration and Open Science. This ensures scientists are being fair and open with their methods. The process involves: specify what you want to do, ensure a plan of action, do not hypothesize after collecting results, justify your sample size, IV, and DV, and describe the process. This helps eliminate p-hacking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e3750",
   "metadata": {},
   "source": [
    "Advanced: If we do not achieve our alpha critical value, then we can conduct an equivalence test, bayes factors, or Bayesian estimation to determine if there was an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d13cd",
   "metadata": {},
   "source": [
    "<b> Bonus: Return of the p-values: P-Value Debate and P-Hacking </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d5e35",
   "metadata": {},
   "source": [
    "p-hacking refers to manipulating statistics and hypotheses to find significance. This is facilitated by people wanting to find meaningful differences in their data. However, this is bad because the analyzer is biased by the data, and it can lead to false findings in publications. Scientists must work together to optimize the confidence of their findings for the general use. The issue is data collection is complex. Anyone with a basic understanding can manipulate their design, data, analysis, or assumptions into being superficially accepted as \"good evidence.\"\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=HDCOUXE3HMM&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=13\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=Zpu3VtnRhLw (This is a great channel for statistical philosophy)\n",
    "\n",
    "HARKING: Hypothesizing After Results are Known is not always considered p-hacking, but it is argued that HARKING is not testing a hypothesis, because you already know the data before the test.\n",
    "\n",
    "\"There are lies, damned lies, and statistics... Facts are stubborn, but statistics are more pliable.\" - Mark Twain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4cb52b",
   "metadata": {},
   "source": [
    "Frequentist statistics is characterized by its use of p-values. You will not find this in other areas such as Bayesian statistics. The question is, do p-values make sense in statistics? Hopefully this document explains these concepts clearly enough that you think statistics are okay. I generally view they are very useful, partially because they are the best we have, but also because they do give a decent view of what is going on. Next, we will get into the debate over p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c1bae",
   "metadata": {},
   "source": [
    "In summary, Fisher suggested that p-values were a continuous measure of significance. Whereas Neyman-Pearson suggested p-values should have a fixed level.\n",
    "\n",
    "To contextualize this let us look at the p-values of test 1 (p=.049) and test 2 (p=.051).\n",
    "\n",
    "Fisher would interpret this as having roughly the same amount of significance.\n",
    "\n",
    "Neyman-Pearson woould suggest that only test 1 matters.\n",
    "\n",
    "Article: https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00223/full\n",
    "\n",
    "This is a pretty serious matter because so much of our statistics revolves around p-values. I generally observe a Neyman-Pearson approach in formal work. However, I think Fisher does have some important points here. Consider a intentionally exaggerated medical situation. The contemporary treatment gives people a 95% chance of living. Now you find a new interventin where you find a p-value of .01. If you conduct this treatment on 100 patients, 1 of them would die from the treatment being a misdiganoses. Do you say the life of the 99 is better than having no treatment at all, despite the 1 being unjustly diagnosed due to error in the intervention? I think these ideas are really well postulated in Harvard's course of Justice (episode 1) available on Youtube. \n",
    "\n",
    "Video 1: https://www.youtube.com/watch?v=bf3egy7TQ2Q&list=PLH2l6uzC4UEW3iJO4T0qUeUEp_X-f1U7S&index=22&pp=iAQB\n",
    "\n",
    "Video 2: https://www.youtube.com/watch?v=WWagtGT1zH4&list=PLH2l6uzC4UEW3iJO4T0qUeUEp_X-f1U7S&index=24&pp=iAQB\n",
    "\n",
    "\"... the actual and physical conduct of an experiment must govern the statistical procedure of its interpretation.\" R. A. Fisher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f71eec",
   "metadata": {},
   "source": [
    "<b> Bonus: Ethical P-Interpretation </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985a5d6",
   "metadata": {},
   "source": [
    "1.\tP values tell you about DATA, not theories.\n",
    "2.\tMisinterpretation 1: A non-significant p-value means the null hypothesis is true.\n",
    "a.\tP values tell you when something is surprising is in the data, not when a hypothesis is correct or incorrect.\n",
    "3.\tMisinterpretation 2: A significant p value means that the null hypothesis is false.\n",
    "a.\tIt is best to say “Statistical significance was found with the understanding that there is a 5% chance of error for a type 2 error.”\n",
    "4.\tMisinterpretation 3: A significant p value means there is practical importance.\n",
    "a.\tTo know if an effect matters (significance, correlation, etc), you need to conduct a cost benefit analysis. \n",
    "5.\tMisinterpretation 4: If you have observed a significant finding, the probability that you have made a Type 1 error is 5%. \n",
    "a.\tNote the difference between; what is the probability that the null hypothesis is true if I have observed p < .05; what is the probability of observing this or more extreme data?\n",
    "b.\tThe difference is that the first question assumes the null hypothesis is true prior to collecting data.\n",
    "6.\tMister interpretation 5: 1-p = the probability an effect will be replicated. \n",
    "a.\tProbability for detecting significance is determined by power, not p value. \n",
    "\n",
    "\n",
    "More info: Greenland, Senn, Rothman, Carlin, Poole, Goodman, and Altman (2016), Making Null Meaningful (Harms & Lakens, 2018)\n",
    "\n",
    "Although p-values do not tell us what is truth or even the nature of a theory. We can derive some support using a p-curve analysis. This posits the question, does a set of p-values have evidential value. To do this, we take the p-values from a line of literature or studies and plot them to determine if there is an effect. See \"p-curve.com\" as an example. This can also tell you something about the reliability of a series of studies.\n",
    "\n",
    "Good Practices:\n",
    "It is recommended that you do truncate or round your statistical values. This overestimates or underestimates the observed representation of the population. \n",
    "\n",
    "It is also recommended that you are careful with \"trimming\" your data. Typically this falls around the idea of Dixon's Q test, which identifies whether a value is an outlier within its distribution. If one finds an outlier, they have three options: trim it (delete it), winsorize it (replace it with a percentile value), or do nothing. I encourage you to do nothing, because in my experience it is not appropriate to edit the data to meet some expectation. However, it is your job to investigate these topics and determine where you fall on this ethics issue. \n",
    "\n",
    "\"If ... we choose a group of social phenomena with no antecedent knowledge of the causation or absence of causation among them, then the calculation of correlation coefficients, total or partial, will not advance us a step toward evaluating the importance of the causes at work.\" R. A. Fisher\n",
    "\n",
    "Video 3: https://www.youtube.com/watch?v=PPD8lER8ju4&list=PLH2l6uzC4UEW3iJO4T0qUeUEp_X-f1U7S&index=23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32535a",
   "metadata": {},
   "source": [
    "<b> Standard Error and Bootstrapping </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82df5b",
   "metadata": {},
   "source": [
    "Standard error describes how different a sample mean may be from a population mean. This leads into the chi-square analysis in the next section. Standard error is simple to calculate, SE = SDofSample / SquareRoot(sample size). When we have several samples and their means, and take the mean of the means, we can also get a standard deviation of those means. However, in this case we refer to it as the standard error. It is useful when quantifying the variation of multiple measures. It gives a sense of variation among groups of samples. You can use bootstrapping to simulate this instead of rerunning the experiment. \n",
    "\n",
    "Video: https://www.youtube.com/watch?v=XNgt7F6FqDU&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=23\n",
    "\n",
    "Boot strapping allows you to determine if a measurement is meaningful without collecting more data, by sampling from the distribution of the data you have. This is contended some depending on the context in which it is being used. For example, one flaw is it confirms what it already has and it does not account for issues like temporal precedence. However, it is useful when you know what the population distribution should look like because bootstrapping will allow you to generate more data (of any data type). \n",
    "\n",
    "Video: https://www.youtube.com/watch?v=Xz0x-8-cgaQ&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=25\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=N4ZQQqyIf6k&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=26\n",
    "\n",
    "Reading: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4784504/\n",
    "\n",
    "Reading: https://www.linkedin.com/advice/1/what-advantages-disadvantages-bootstrapping-data\n",
    "\n",
    "In summary, Bootstrapping replicates data statistically instead of methodologically. You can do this with sampling with replacement, or having duplicates, so each sample is not the exact same. Note that a new dataset made from bootstrapping is called a bootstrapped dataset. You can get p values to compare bootstrapped data (when checking for random error). You can bootstrap using medians if there were outliers for example (where medians are resilient to outliers) or any other metric. \n",
    "\n",
    "\"All life is an experiment. The more experiments you make, the better.\" Ralph Waldo Emerson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7405d84b",
   "metadata": {},
   "source": [
    "<b> Effect Sizes </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728df44",
   "metadata": {},
   "source": [
    "Cohen (1988) provided a statistical for measuring the amount of \"effect\" in an analysis. This leads us to our material on General Linear Models (in part 3) but for now, we will focus on the concept of the variable itself. Cohen's d (Cohen, 1988) suggested on a scale of 0-1, effect sizes near .2 are small, .5 are medium, and .8 are large. Cohen's d is designed for comparing two groups. It takes the difference between two means and expresses it in standard deviation units. It tells you how many standard deviations lie between the two means.\n",
    "\n",
    "However, there is naturally some debate over how arbitrary these values are and how they should be interpreted (Thompson, 2007). Alas, Cohen's d is a popular metric for computing effect size. It is also worth noting the following alternatives as Cohen's d is often criticized for slightly overestimating effect size. It is common to describe forms of an analysis as liberal or conservative.\n",
    "\n",
    "Hedge's g: an unbiased version of cohen's d. This is best when the two samples are not equal in sample size.\n",
    "\n",
    "Cohen's f: Best when used in a t-test, one way ANOVA.\n",
    "\n",
    "Cohen's f^2: Best when used in multiple regression. \n",
    "\n",
    "Glass's delta: When SD's are significantly different between the groups.\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=WMTxyWq4E2M\n",
    "\n",
    "Video 2: https://www.youtube.com/watch?v=sMc5eX4OKbI\n",
    "\n",
    "Video 3: https://www.youtube.com/watch?v=ISJqVcKZyLs\n",
    "\n",
    "You may also see Omega Squared, Partial Omega Squared, and Eta Squared. These are effect size measures for ANOVA that measures the strength of associated between catagorical independent variables and dependent variables.\n",
    "\n",
    "Reading: https://lbecker.uccs.edu/glm_effectsize\n",
    "\n",
    "Reading: Effect Sizes Overview: Grissom, R. J., & Kim, J. J. (2012). Effect sizes for research: univariate and multivariate applications (2nd ed). New York: Routledge.\n",
    "\n",
    "Reading: Cohen’s d: Cumming, G. (2013). Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis. Routledge.\n",
    "\n",
    "Reading: Practical Effect Size Primer: Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs. Frontiers in Psychology, 4. http://doi.org/10.3389/fpsyg.2013.00863\n",
    "\n",
    "Reading (Advanced): Contrasts Practical Introduction: Rosnow, R. L., & Rosenthal, R. (2009). Effect Sizes: Why, When, and How to Use Them. Zeitschrift Für Psychologie / Journal of Psychology, 217(1), 6–14. http://doi.org/10.1027/0044-3409.217.1.6\n",
    "\n",
    "Reading (Advanced): Book on Effect Size: Ellis, P. D. (2010). The essential guide to effect sizes: statistical power, meta-analysis, and the interpretation of research results. Cambridge ; New York: Cambridge University Press.\n",
    "\n",
    "\n",
    "\"[Statistics are] the only tools by which an opening can be cut through the formidable thicket of difficulties that bars the path of those who pursue the science of man.\" Sir Francis Galton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe4e1f",
   "metadata": {},
   "source": [
    "<b> Bonus: Confidence Intervals </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e7b65",
   "metadata": {},
   "source": [
    "Confidence intervals are simply ranges where 95% of the values in the data are. Anything outside of this is less than p<.05. This means anything outside is significantly different. If two confidence intervals overlap, you should use a test, but if they do not then they are significantly different. \n",
    "\n",
    "Video: https://www.youtube.com/watch?v=TqOeMYtOc1w&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=24\n",
    "\n",
    "Reading: Confidence Intervals Explained: Cumming, G., & Finch, S. (2001). A Primer on the Understanding, Use, and Calculation of Confidence Intervals that are Based on Central and Noncentral Distributions. Educational and Psychological Measurement, 61(4), 532–574. http://doi.org/10.1177/0013164401614002\n",
    "\n",
    "Reading: CI continued: Cumming, G., & Fidler, F. (2009). Confidence Intervals: Better Answers to Better Questions. Zeitschrift Für Psychologie / Journal of Psychology, 217(1), 15–26. http://doi.org/10.1027/0044-3409.217.1.15\n",
    "\n",
    "Reading: Confidence Intervals from a Bayesian Perspective: Morey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of placing confidence in confidence intervals. Psychonomic Bulletin & Review, 23(1), 103–123.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23140a",
   "metadata": {},
   "source": [
    "<b> Bonus: Degrees of Freedom </b>\n",
    "\n",
    "Degrees of Freedom (Df) are used to find critical thesholds or values for establishing p-values. They are calculated using n (population) and n-1 (sample). Computational methods often do this for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17973424",
   "metadata": {},
   "source": [
    "<b> Advanced: Sequential Analysis Corrections </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089956e",
   "metadata": {},
   "source": [
    "The is a post hoc analysis used to correct p-values when there is a sequence of tests being conducted on the data. This may be important because when doing sequential analyses, you may be inflating your error rate. For example, you analyze data during data collection, so the sample size is not fixed. You would want to avoid p-hacking by prematurally analyzing data, however this is sometimes neccessary. While I will not walk through these tests here, I will list some examples. \n",
    "\n",
    "Reading: How to conduct sequential analysis: Lakens, D. (2014). Performing high-powered studies efficiently with sequential analyses: Sequential analyses. European Journal of Social Psychology, 44(7), 701–710. http://doi.org/10.1002/ejsp.2023\n",
    "\n",
    "Bonferonni Correction: The most historical, but many others have built on it. \n",
    "\n",
    "Holm-Bonferonni Correction (Sequentially Rejective Bonferonni Test): \n",
    "\n",
    "Benjamini-Hochberg Correction:\n",
    "\n",
    "Pocock Correction: Popular in clinical settings. \n",
    "\n",
    "O'Brien-Fleming Correction: Popular in clinical settings.\n",
    "\n",
    "Haybittle-Peto Correction:  \n",
    "\n",
    "Sidak Correction:\n",
    "\n",
    "Tukey Correction:\n",
    "\n",
    "Holm's Step Procedure:\n",
    "\n",
    "Hochberg's Step Procdure:\n",
    "\n",
    "Q-Statistic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894258ca",
   "metadata": {},
   "source": [
    "Advanced: If you want a strong understanding of these concepts, I encourage you to skip around Dr. Daniel Lakens \"Improving Your Statistical Inferences\" and \"Improving Your Statistical Hypotheses\" on Coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a38d9",
   "metadata": {},
   "source": [
    "<b> Conclusion </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599af7f",
   "metadata": {},
   "source": [
    "This is the end of Episode II. Hopefully you have a strong understanding of alpha, beta, p-values, and a few other concepts that are generally important to statistics. You also covered advanced topics like Sequential Analysis Corrections and the underlying philosophy behind p-values. One of the really cool things in statistics is you do not have to have years of experience to dive into the philosphical bases for this material; none the less there are certainly much more complex topics. It is my hope you have been equipped with the tools and terms needed to conversate on these things. In Episode III, I will be using GLMs (general linear models) to conduct statistical tests.\n",
    "\n",
    "Bonus Philosophy of Science Readings:\n",
    "\n",
    "Reading: Philosophy of Science: Meehl, P. E. (1990). Appraising and amending theories: The strategy of Lakatosian defense and two principles that warrant it. Psychological Inquiry, 1(2), 108–141.\n",
    "\n",
    "Reading: PhilSci: Hull, D. L. (2010). Science as a process: an evolutionary account of the social and conceptual development of science. University of Chicago Press. \n",
    "\n",
    "Reading: PhilSci: Ladyman, J. (2002). Understanding philosophy of science. London; New York: Routledge.\n",
    "\n",
    "Reading: PhilSci: Godfrey-Smith, P. (2003). Theory and reality: an introduction to the philosophy of science. Chicago: University of Chicago Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

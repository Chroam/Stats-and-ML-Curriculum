{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148fca3b",
   "metadata": {},
   "source": [
    "GPT: \"What is a neural network?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b759ba5f",
   "metadata": {},
   "source": [
    "A neural network is a computational model inspired by the way biological neural networks in the human brain operate. It is composed of interconnected nodes, also known as neurons or artificial neurons, organized in layers. Neural networks are a fundamental component of deep learning, a subfield of machine learning.\n",
    "\n",
    "Key Components of a Neural Network:\n",
    "\n",
    "Input Layer:\n",
    "\n",
    "The layer that receives input features or data.\n",
    "\n",
    "Hidden Layers:\n",
    "\n",
    "Intermediate layers between the input and output layers where computations are performed. Deep neural networks have multiple hidden layers.\n",
    "\n",
    "Weights and Biases:\n",
    "\n",
    "Parameters that the neural network learns from training data. They are adjusted during the training process to optimize the network's performance.\n",
    "\n",
    "Activation Function:\n",
    "\n",
    "Non-linear functions applied to the output of each neuron in a layer. They introduce non-linearity to the model, enabling it to learn complex relationships in the data.\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "The layer that produces the final output or prediction.\n",
    "\n",
    "How Neural Networks Work:\n",
    "\n",
    "Forward Propagation:\n",
    "\n",
    "During forward propagation, input data is passed through the network layer by layer, and computations are performed using weights, biases, and activation functions to generate predictions.\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "A loss function measures the difference between the predicted output and the actual target. The goal during training is to minimize this loss.\n",
    "\n",
    "Backward Propagation (Training):\n",
    "\n",
    "Backward propagation involves updating the weights and biases based on the gradients of the loss function with respect to the parameters. This is done using optimization algorithms like gradient descent.\n",
    "\n",
    "Training Epochs:\n",
    "\n",
    "The entire process of forward and backward propagation is repeated for multiple iterations or epochs until the model achieves satisfactory performance on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83041f44",
   "metadata": {},
   "source": [
    "There are four initial types of neural nets to learn about. First is Feedforward Neural Network (FNN). In this notebook, I will not use packages such as tensorflow and Keras. These require specific environment setups, and its much more diffucult to see the underlying algorithm. Be sure to comment these algorithms line by line if anything is confusing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58650846",
   "metadata": {},
   "source": [
    "GPT: \"Write a FNN from scratch and apply it to the Iris dataset with thorough comments.\" I also added some more information to each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae9da156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 154.7865425456444\n",
      "Epoch 1000, Loss: 56.510517792337055\n",
      "Epoch 2000, Loss: 36.71469653214517\n",
      "Epoch 3000, Loss: 28.151420208129704\n",
      "Epoch 4000, Loss: 32.66388458843728\n",
      "Epoch 5000, Loss: 30.732255348136125\n",
      "Epoch 6000, Loss: 27.343314460242798\n",
      "Epoch 7000, Loss: 21.953026188244053\n",
      "Epoch 8000, Loss: 31.204677792416714\n",
      "Epoch 9000, Loss: 17.575426356629414\n",
      "Accuracy: 70.0%\n"
     ]
    }
   ],
   "source": [
    "#Feedforward Neural Networks (FNNs) are a type of artificial neural network where information moves \n",
    "#in one direction—from the input layer through hidden layers to the output layer—without cycles or loops.\n",
    "#FNN is good for structured problems, featured engineering, simple to complex mapping, supervised and labeled\n",
    "#data, scalable problems. \n",
    "\n",
    "#FNNs are not ideal for image recognition, unsupervised data,sequential data like time-series, transfer learning, \n",
    "#factorization machines, collaborative filtering. \n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Standardize features to have zero mean and unit variance, NOTICE how it is 70% accuracy without standardization\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the labels for classification\n",
    "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "# One-hot encoding is used to convert categorical labels into a binary matrix.\n",
    "# Each unique label is represented by a binary vector with all zeros except for a one at the index corresponding to the label.\n",
    "# In this case, it transforms the single-column array 'y' into a binary matrix 'y_onehot'.\n",
    "#This is effectively dummy coding. \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural Network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "# The weights represent the strength of connections between neurons in different layers.\n",
    "        # The biases provide flexibility and allow the network to learn offset values for each neuron.\n",
    "        # Random initialization helps break symmetry and allows the network to start learning unique features.\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    " # The sigmoid function squashes the input values between 0 and 1, making them suitable for binary classification problems.\n",
    "        # It introduces non-linearity to the network, allowing it to learn complex patterns.\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "# The derivative of the sigmoid function is used in the backward pass (backpropagation) to compute gradients.\n",
    "        # It helps determine the contribution of each neuron to the error, facilitating weight updates.\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Softmax activation function for multi-class classification\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "# The softmax function is used to convert the raw output scores into probabilities for multi-class classification.\n",
    "        # It ensures that the sum of probabilities for each data point is 1, making it a valid probability distribution.\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #  Forward pass through the network. This is the predicted output.\n",
    "        self.hidden_output = self.sigmoid(np.dot(X, self.weights_input_hidden) + self.bias_hidden)\n",
    "        self.final_output = self.softmax(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output)\n",
    "        return self.final_output\n",
    "# The forward pass computes the output of the network given the input.\n",
    "        # It involves matrix multiplications with weights and adding biases, followed by activation functions.\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        #Backward pass to update weights and biases based on the error\n",
    "        #The backward pass computes the gradients of the loss with respect to the parameters (weights and biases) \n",
    "        #of the network. It allows the network to understand how each parameter contributed to the error and \n",
    "        #updates the parameters accordingly.\n",
    "        error = y - self.final_output\n",
    "        output_delta = error * self.sigmoid_derivative(self.final_output)\n",
    "        # Compute the error at the final output layer and the gradient of the error with respect to the output\n",
    "        error_hidden = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = error_hidden * self.sigmoid_derivative(self.hidden_output)\n",
    "        # The backward pass computes the gradients of the loss with respect to weights and biases.\n",
    "        # It uses these gradients to update the weights and biases using gradient descent.\n",
    "        # Backpropagate the error to the hidden layer by computing the gradient of the error with respect to the hidden layer output\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "        # Updating weights and biases involves adjusting them in the direction that reduces the loss.\n",
    "        # Learning rate controls the size of these adjustments.\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        # Train the neural network for a specified number of epochs\n",
    "        for epoch in range(epochs):\n",
    "            # Perform a forward pass and a backward pass for each epoch\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            # Print the loss every 1000 epochs for monitoring\n",
    "            if epoch % 1000 == 0:\n",
    "                loss = -np.sum(y * np.log(output))\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the trained neural network\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize neural network with input size, hidden size, and output size\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 5\n",
    "output_size = y_train.shape[1]\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the neural network with the training data\n",
    "nn.train(X_train, y_train, epochs=10000, learning_rate=0.01)\n",
    "\n",
    "# Test the neural network on the testing data\n",
    "predictions = nn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the model\n",
    "accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
    "print(f'Accuracy: {accuracy * 100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a71662",
   "metadata": {},
   "source": [
    "<b> Convultional Neural Netowork (CNN) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44b1fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ive spent several hours on this and cannot get it to work. Maybe try this on GPT4 or whatever when I can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5986ade4",
   "metadata": {},
   "source": [
    "CNN, RNN, LSTM, Factorization and Colaborative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

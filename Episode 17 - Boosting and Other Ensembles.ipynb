{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf189db",
   "metadata": {},
   "source": [
    "GPT: \"What are the popular types of ensemble methods?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9214b54",
   "metadata": {},
   "source": [
    "Random Forests\n",
    "\n",
    "Type: Bagging (Bootstrap Aggregating)\n",
    "\n",
    "How it works: Constructs multiple decision trees during training and outputs the mean prediction (for regression problems) or majority vote (for classification problems) of the individual trees.\n",
    "\n",
    "Advantages: Robust to overfitting, handles high-dimensional data well, provides feature importance.\n",
    "\n",
    "Gradient Boosting\n",
    "\n",
    "Type: Boosting\n",
    "\n",
    "How it works: Builds multiple weak learners sequentially, with each one correcting errors made by the previous models. It minimizes a loss function during training.\n",
    "\n",
    "Advantages: Generally more accurate than AdaBoost, handles complex relationships well, supports various loss functions.\n",
    "\n",
    "AdaBoost (Adaptive Boosting)\n",
    "\n",
    "Type: Boosting\n",
    "\n",
    "How it works: Assigns weights to data points and trains weak learners sequentially. Focuses on misclassified data points, giving them higher weights.\n",
    "\n",
    "Advantages: Effective in combining weak learners, can achieve high accuracy, less prone to overfitting.\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "Type: Boosting\n",
    "\n",
    "How it works: An efficient and scalable implementation of gradient boosting. It includes regularization terms and a more advanced optimization algorithm.\n",
    "\n",
    "Advantages: Improved performance and speed, handles missing data, parallelizable, supports regularization.\n",
    "\n",
    "LightGBM\n",
    "\n",
    "Type: Boosting\n",
    "\n",
    "How it works: A gradient boosting framework that uses tree-based learning algorithms. It partitions data points vertically (leaf-wise) rather than horizontally (level-wise).\n",
    "\n",
    "Advantages: Faster training times, lower memory usage, good for large datasets.\n",
    "\n",
    "CatBoost\n",
    "\n",
    "Type: Boosting\n",
    "\n",
    "How it works: A gradient boosting algorithm that is designed to handle categorical features efficiently. It uses a variant of the gradient boosting algorithm with ordered boosting.\n",
    "\n",
    "Advantages: Automatically handles categorical features, robust to overfitting, handles missing values.\n",
    "\n",
    "Ensemble of Classifiers (Voting Classifier)\n",
    "\n",
    "Type: Various\n",
    "\n",
    "How it works: Combines multiple classifiers (e.g., Logistic Regression, Decision Trees, Support Vector Machines) and makes predictions based on majority voting or averaging.\n",
    "\n",
    "Advantages: Can improve overall performance by leveraging the strengths of different models.\n",
    "\n",
    "Stacking\n",
    "\n",
    "Type: Various\n",
    "\n",
    "How it works: Trains multiple models and combines their predictions using another model (meta-model). The base models can be diverse algorithms.\n",
    "\n",
    "Advantages: Can capture different patterns learned by individual models, potentially leading to improved performance.\n",
    "Choosing the right ensemble method depends on the characteristics of your data and the specific goals of your machine learning task. Each method has its strengths and weaknesses, and experimentation is often necessary to determine the most effective approach for a given problem.\n",
    "\n",
    "As an extra assignment, go through each technique here and describe the disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2c77b",
   "metadata": {},
   "source": [
    "<b> Gradient Boost </b> compared to an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978dbcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Gradient Boosting:\n",
      "Accuracy: 0.97\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  1]\n",
      " [ 2 68]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        44\n",
      "           1       0.99      0.97      0.98        70\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Results for Support Vector Machine (SVM):\n",
      "Accuracy: 0.96\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  1]\n",
      " [ 4 66]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95        44\n",
      "           1       0.99      0.94      0.96        70\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.95      0.96      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=47)\n",
    "\n",
    "# Step 1: Create a Gradient Boosting classifier\n",
    "gradient_boosting = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=4)\n",
    "\n",
    "# Step 2: Train the Gradient Boosting classifier on the training data\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions using the Gradient Boosting model\n",
    "y_pred_gradient_boosting = gradient_boosting.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the performance of Gradient Boosting\n",
    "accuracy_gradient_boosting = accuracy_score(y_test, y_pred_gradient_boosting)\n",
    "conf_matrix_gradient_boosting = confusion_matrix(y_test, y_pred_gradient_boosting)\n",
    "class_report_gradient_boosting = classification_report(y_test, y_pred_gradient_boosting)\n",
    "\n",
    "# Step 5: Create a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1, random_state=42)\n",
    "\n",
    "# Step 6: Train the SVM classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions using the SVM model\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the performance of SVM\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "class_report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "# Print the results for both models\n",
    "print(\"Results for Gradient Boosting:\")\n",
    "print(f\"Accuracy: {accuracy_gradient_boosting:.2f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_gradient_boosting)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report_gradient_boosting)\n",
    "\n",
    "print(\"\\n------------------------------------\\n\")\n",
    "\n",
    "print(\"Results for Support Vector Machine (SVM):\")\n",
    "print(f\"Accuracy: {accuracy_svm:.2f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_svm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cfb4c",
   "metadata": {},
   "source": [
    "Here we see the gradient boost performed better in a classification task. While this may seem like a negligable difference, consider how .01 improvement could be important over the course of thousands or more data units, or in highly sensitive situations such as healthcare.\n",
    "\n",
    "Next, compare Gradient Boost and Adaboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8d71ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Gradient Boosting:\n",
      "Accuracy: 0.96\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  3]\n",
      " [ 2 69]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        43\n",
      "           1       0.96      0.97      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Results for AdaBoost:\n",
      "Accuracy: 0.97\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  2]\n",
      " [ 1 70]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Create a Gradient Boosting classifier\n",
    "gradient_boosting = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Step 2: Train the Gradient Boosting classifier on the training data\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions using the Gradient Boosting model\n",
    "y_pred_gradient_boosting = gradient_boosting.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the performance of Gradient Boosting\n",
    "accuracy_gradient_boosting = accuracy_score(y_test, y_pred_gradient_boosting)\n",
    "conf_matrix_gradient_boosting = confusion_matrix(y_test, y_pred_gradient_boosting)\n",
    "class_report_gradient_boosting = classification_report(y_test, y_pred_gradient_boosting)\n",
    "\n",
    "# Step 5: Create an AdaBoost classifier\n",
    "adaboost = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Step 6: Train the AdaBoost classifier on the training data\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions using the AdaBoost model\n",
    "y_pred_adaboost = adaboost.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the performance of AdaBoost\n",
    "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
    "conf_matrix_adaboost = confusion_matrix(y_test, y_pred_adaboost)\n",
    "class_report_adaboost = classification_report(y_test, y_pred_adaboost)\n",
    "\n",
    "# Print the results for both models\n",
    "print(\"Results for Gradient Boosting:\")\n",
    "print(f\"Accuracy: {accuracy_gradient_boosting:.2f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_gradient_boosting)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report_gradient_boosting)\n",
    "\n",
    "print(\"\\n------------------------------------\\n\")\n",
    "\n",
    "print(\"Results for AdaBoost:\")\n",
    "print(f\"Accuracy: {accuracy_adaboost:.2f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_adaboost)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report_adaboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d04d3",
   "metadata": {},
   "source": [
    "Next let's compare XGBoost and Gradient Boost, this time for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d9c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-2.0.2-py3-none-win_amd64.whl (99.8 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\public\\anaconda\\lib\\site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\public\\anaconda\\lib\\site-packages (from xgboost) (1.10.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf879721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Gradient Boosting: 0.29\n",
      "Mean Squared Error for XGBoost: 0.28\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=64)\n",
    "\n",
    "# Step 1: Create a Gradient Boosting regressor\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=24)\n",
    "\n",
    "# Step 2: Train the Gradient Boosting regressor on the training data\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions using the Gradient Boosting model\n",
    "y_pred_gradient_boosting = gradient_boosting.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the performance of Gradient Boosting using Mean Squared Error\n",
    "mse_gradient_boosting = mean_squared_error(y_test, y_pred_gradient_boosting)\n",
    "print(f\"Mean Squared Error for Gradient Boosting: {mse_gradient_boosting:.2f}\")\n",
    "\n",
    "# Step 5: Create an XGBoost regressor\n",
    "xgboost = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=4)\n",
    "\n",
    "# Step 6: Train the XGBoost regressor on the training data\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions using the XGBoost model\n",
    "y_pred_xgboost = xgboost.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the performance of XGBoost using Mean Squared Error\n",
    "mse_xgboost = mean_squared_error(y_test, y_pred_xgboost)\n",
    "print(f\"Mean Squared Error for XGBoost: {mse_xgboost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f5af9",
   "metadata": {},
   "source": [
    "Here we see the XGBoost outperformed, it also took advantage of things like its ability to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2e2910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Using cached catboost-1.2.2-cp38-cp38-win_amd64.whl (101.1 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\public\\anaconda\\lib\\site-packages (from catboost) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\public\\anaconda\\lib\\site-packages (from catboost) (1.24.4)\n",
      "Collecting plotly\n",
      "  Using cached plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\preston menke\\appdata\\roaming\\python\\python38\\site-packages (from catboost) (3.7.1)\n",
      "Requirement already satisfied: six in c:\\users\\public\\anaconda\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\public\\anaconda\\lib\\site-packages (from catboost) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\public\\anaconda\\lib\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\public\\anaconda\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\public\\anaconda\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\preston menke\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\public\\anaconda\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\preston menke\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->catboost) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\public\\anaconda\\lib\\site-packages (from matplotlib->catboost) (8.3.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\public\\anaconda\\lib\\site-packages (from matplotlib->catboost) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\preston menke\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\public\\anaconda\\lib\\site-packages (from matplotlib->catboost) (23.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\preston menke\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->catboost) (4.39.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\public\\anaconda\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.5.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\public\\anaconda\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
      "Installing collected packages: plotly, graphviz, catboost\n",
      "Successfully installed catboost-1.2.2 graphviz-0.20.1 plotly-5.18.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8b7480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Gradient Boosting: 0.29\n",
      "Mean Squared Error for CatBoost: 0.27\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Create a Gradient Boosting regressor\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Step 2: Train the Gradient Boosting regressor on the training data\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions using the Gradient Boosting model\n",
    "y_pred_gradient_boosting = gradient_boosting.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the performance of Gradient Boosting using Mean Squared Error\n",
    "mse_gradient_boosting = mean_squared_error(y_test, y_pred_gradient_boosting)\n",
    "print(f\"Mean Squared Error for Gradient Boosting: {mse_gradient_boosting:.2f}\")\n",
    "\n",
    "# Step 5: Create a CatBoost regressor\n",
    "catboost = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, random_seed=42, verbose=0)\n",
    "\n",
    "# Step 6: Train the CatBoost regressor on the training data\n",
    "catboost.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions using the CatBoost model\n",
    "y_pred_catboost = catboost.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the performance of CatBoost using Mean Squared Error\n",
    "mse_catboost = mean_squared_error(y_test, y_pred_catboost)\n",
    "print(f\"Mean Squared Error for CatBoost: {mse_catboost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150ffc2",
   "metadata": {},
   "source": [
    "We may not always get better results with the more advanced boosting techniques. This is why it is important to recognize the appropriate times for each boosting method. \n",
    "\n",
    "Similarly, you may find ensembles of ensemble methods, a meta-model, and a technique known as stacking, can be used to combine multiple method. However, techniques such as this treat error as additive, so you may introduce noise that is avoidable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44f85db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Stacking Classifier: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Create base models\n",
    "base_models = [\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gradient_boosting', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)),\n",
    "    ('adaboost', AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)),\n",
    "    ('svm', SVC(kernel='linear', C=1, probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "# Step 2: Create a meta-model (stacking classifier)\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Step 3: Create the stacking classifier\n",
    "stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Step 4: Train the stacking classifier on the training data\n",
    "stacking_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions using the stacking classifier\n",
    "y_pred_stacking = stacking_classifier.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the performance of the stacking classifier\n",
    "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "print(f\"Accuracy for Stacking Classifier: {accuracy_stacking:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfb24e",
   "metadata": {},
   "source": [
    "In these scenarios, you may get an accuracy greater than any individual ensemble method, however, you still need to recognize phenomena such as overfitting and introducing error. The following section is a bit of a lengthy read, but I found it a very useful comparison of Bagging and Boosting.\n",
    "\n",
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14531aab",
   "metadata": {},
   "source": [
    "Just to clarify the difference between Baggin and Boosting.\n",
    "\n",
    "Bagging (Bootstrap Aggregating) and Boosting are both ensemble learning techniques, but they differ in their approach to combining multiple models.\n",
    "\n",
    "<b>Bagging (Bootstrap Aggregating)</b>\n",
    "\n",
    "Concept:\n",
    "\n",
    "Bagging involves training multiple instances of the same learning algorithm on different subsets of the training data.\n",
    "Each subset is created by sampling with replacement from the original training data, a process known as bootstrapping.\n",
    "\n",
    "Training Process:\n",
    "\n",
    "Each model is trained independently, and there is no interaction between the models during training.\n",
    "Models are trained in parallel, making bagging suitable for parallel or distributed computing.\n",
    "\n",
    "Model Diversity:\n",
    "\n",
    "Bagging aims to reduce variance and improve stability by averaging the predictions of multiple models.\n",
    "The diversity among models comes from the different subsets of the training data they are exposed to.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Random Forest is a popular bagging algorithm that builds an ensemble of decision trees.\n",
    "\n",
    "<b>Boosting </b>\n",
    "\n",
    "Concept:\n",
    "\n",
    "Boosting involves sequentially training multiple weak learners (models that perform slightly better than random chance) and giving more emphasis to instances that were misclassified by previous models.\n",
    "\n",
    "Training Process:\n",
    "\n",
    "Each model is trained sequentially, and the training of a new model depends on the performance of the previous ones.\n",
    "Models are trained to correct the errors made by the ensemble up to that point.\n",
    "\n",
    "Model Diversity:\n",
    "\n",
    "Boosting aims to reduce bias and improve accuracy by focusing on difficult-to-classify instances.\n",
    "The diversity among models comes from their sequential nature, with each model attempting to correct the mistakes of its predecessors.\n",
    "\n",
    "Examples:\n",
    "\n",
    "AdaBoost (Adaptive Boosting) and Gradient Boosting are common boosting algorithms.\n",
    "\n",
    "<b>Differences </b>\n",
    "\n",
    "Parallel vs. Sequential:\n",
    "\n",
    "Bagging trains models independently in parallel.\n",
    "Boosting trains models sequentially, with each model dependent on the performance of the previous ones.\n",
    "\n",
    "Model Interaction:\n",
    "\n",
    "Bagging models are independent and don't interact during training.\n",
    "Boosting models are trained to correct errors made by the ensemble, leading to sequential dependencies.\n",
    "\n",
    "Emphasis on Errors:\n",
    "\n",
    "Bagging aims to reduce variance and improve stability.\n",
    "Boosting aims to reduce bias and improve accuracy by focusing on difficult-to-classify instances.\n",
    "In summary, bagging creates diverse models by training them independently on different subsets of data, while boosting creates diversity by sequentially training models to correct errors made by the ensemble. Both approaches aim to improve the overall performance of the ensemble compared to individual models.\n",
    "___\n",
    "___\n",
    "___\n",
    "The choice between bagging (Bootstrap Aggregating) and boosting depends on the characteristics of the data, the model, and the goals of the machine learning task. Here are some scenarios where you might prefer bagging over boosting:\n",
    "\n",
    "High Variance, Low Bias:\n",
    "\n",
    "If your base model has high variance (overfitting) and low bias, bagging can be beneficial. Bagging tends to reduce variance by averaging predictions from multiple models trained on different subsets of the data.\n",
    "\n",
    "Parallel Processing:\n",
    "\n",
    "Bagging is well-suited for parallel or distributed computing because each model is trained independently. If you need to scale your computation across multiple processors or machines, bagging may be a more efficient choice.\n",
    "\n",
    "Stability:\n",
    "\n",
    "Bagging is known for its stability and robustness. It is less sensitive to outliers or noisy data points since individual models are trained on different subsets, and their influence on the final ensemble is averaged out.\n",
    "\n",
    "Insensitive to Model Complexity:\n",
    "\n",
    "Bagging is often less sensitive to the choice of the base model's complexity. Even if the base models are overfitting to their respective subsets, the ensemble tends to generalize well.\n",
    "\n",
    "No Sequential Dependencies:\n",
    "\n",
    "If the nature of your problem or the data distribution does not suggest a sequential dependency between models, bagging may be a simpler and more straightforward approach.\n",
    "\n",
    "Robustness to Imbalanced Data:\n",
    "\n",
    "Bagging can handle imbalanced datasets well. If your dataset has imbalanced classes, bagging can help prevent the model from being biased towards the majority class.\n",
    "\n",
    "Lack of Resources for Tuning:\n",
    "\n",
    "Bagging typically requires less hyperparameter tuning compared to boosting. If you have limited resources or time for fine-tuning hyperparameters, bagging might be a more practical choice.\n",
    "Remember that these guidelines are general, and the effectiveness of bagging or boosting can depend on the specific characteristics of your data and the nature of the underlying problem. It's often a good practice to experiment with both approaches and evaluate their performance using cross-validation on your specific dataset.\n",
    "\n",
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "Let's consider a real-life example where bagging might be preferred over boosting:\n",
    "\n",
    "Example: Medical Diagnosis\n",
    "\n",
    "Imagine a scenario where you're building a machine learning model for medical diagnosis, specifically detecting whether a patient has a rare disease based on various medical features.\n",
    "\n",
    "Reasons to Choose Bagging:\n",
    "\n",
    "Data Variability:\n",
    "\n",
    "Medical datasets can be highly variable due to factors such as individual differences, genetic variations, and environmental influences. Bagging, by training models on different subsets of the data, can help create an ensemble that generalizes well across this variability.\n",
    "\n",
    "Model Robustness:\n",
    "\n",
    "In medical diagnosis, it's crucial to have a robust and stable model that doesn't overly rely on specific instances in the dataset. Bagging, by averaging predictions of multiple models, helps create a more robust model less sensitive to outliers or rare cases.\n",
    "\n",
    "High-Dimensional Data:\n",
    "\n",
    "Medical datasets often involve a large number of features, and individual models might overfit to specific features. Bagging can help mitigate overfitting by training models on different subsets of features, improving the generalization of the ensemble.\n",
    "\n",
    "Limited Data Availability:\n",
    "\n",
    "In medical applications, obtaining labeled data for rare diseases might be challenging due to limited occurrences. Bagging, by creating diverse subsets through bootstrapping, allows the model to learn from various perspectives even with limited data.\n",
    "\n",
    "Parallelization:\n",
    "\n",
    "Medical datasets can be large, and training models might be computationally intensive. Bagging, with its parallelizable nature, allows for efficient use of resources in a distributed computing environment.\n",
    "In this example, bagging can provide a more stable and robust model for medical diagnosis, where the dataset is diverse, high-dimensional, and there's a need for a reliable prediction even with limited labeled data for rare cases. The ensemble nature of bagging helps mitigate overfitting and provides a more reliable model for making predictions in a medical context.\n",
    "\n",
    "While boosting is a powerful ensemble learning technique, there are scenarios where it might not be the most appropriate choice for a medical diagnosis task:\n",
    "\n",
    "Sensitive to Outliers:\n",
    "\n",
    "Boosting algorithms, especially those like AdaBoost, are sensitive to outliers in the data. In a medical context, outliers or unusual cases might exist, and boosting could potentially focus too much on correcting misclassifications of these outliers, leading to overfitting.\n",
    "\n",
    "Risk of Overfitting:\n",
    "\n",
    "Boosting aims to correct errors made by previous models in a sequential manner. If there are noise or errors in the training data, boosting might try too hard to fit the training data, leading to overfitting. This is a concern in medical datasets where noise or mislabeling could be present.\n",
    "\n",
    "Data Imbalance:\n",
    "\n",
    "Boosting tends to focus on difficult-to-classify instances, which could be problematic in a medical diagnosis scenario. If the dataset is imbalanced (e.g., rare diseases with fewer instances), boosting might overly emphasize the majority class, leading to biased predictions.\n",
    "\n",
    "Limited Robustness:\n",
    "\n",
    "Boosting may not be as robust as bagging in handling diverse datasets. If the medical dataset is highly variable due to factors like individual differences, genetic variations, or varying environmental conditions, boosting may struggle to generalize well.\n",
    "\n",
    "Sequential Nature:\n",
    "\n",
    "The sequential nature of boosting might not align with the nature of medical data, where various factors contribute simultaneously to a diagnosis. The dependencies between features may not be well captured by a sequential correction of errors.\n",
    "\n",
    "Computational Intensity:\n",
    "\n",
    "Training boosting models can be computationally intensive, especially when dealing with large datasets. In a medical setting where quick and interpretable predictions are often desired, the time and resources required for boosting might be a drawback.\n",
    "It's essential to carefully consider the characteristics of the data and the specific requirements of the medical diagnosis task. While bagging might offer more robustness and stability, the choice between bagging and boosting should be based on empirical testing and evaluation on the specific dataset to determine which technique performs better in terms of accuracy, robustness, and interpretability.\n",
    "\n",
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "Let's consider a real-life example where boosting might be preferred over bagging:\n",
    "\n",
    "Example: Credit Scoring for Loan Approval\n",
    "\n",
    "Imagine a scenario where a financial institution is building a machine learning model to assess the creditworthiness of loan applicants. The goal is to predict whether an applicant is likely to default on a loan based on various financial and personal features.\n",
    "\n",
    "Reasons to Choose Boosting:\n",
    "\n",
    "Imbalanced Data:\n",
    "\n",
    "In credit scoring, the data is often imbalanced because the majority of applicants are expected to have good credit histories, while a smaller proportion might have a higher risk of default. Boosting algorithms, especially those like AdaBoost, are effective in handling imbalanced datasets by giving more weight to misclassified instances (applicants with a higher risk of default).\n",
    "\n",
    "Sequential Learning:\n",
    "\n",
    "Creditworthiness prediction is a task where learning from past mistakes is crucial. Boosting's sequential learning approach allows the model to focus on instances that were misclassified by previous models. This is beneficial in credit scoring, where accurately identifying high-risk applicants is critical.\n",
    "\n",
    "Model Sensitivity to Difficult Cases:\n",
    "\n",
    "Boosting tends to perform well when there are instances that are difficult to classify correctly. In the context of credit scoring, identifying applicants with borderline creditworthiness or those who might have a nuanced risk profile is crucial. Boosting can adapt to these complexities by iteratively improving its ability to handle difficult cases.\n",
    "\n",
    "Fine-Tuning of Weights:\n",
    "\n",
    "Boosting adjusts the weights of misclassified instances during training, placing more emphasis on those instances that are harder to classify. In credit scoring, this can be advantageous because accurately predicting applicants with a higher risk of default is often more important than accurately predicting those with a low risk.\n",
    "\n",
    "Ensemble Diversity:\n",
    "\n",
    "Boosting builds a strong ensemble by sequentially introducing models that correct errors made by previous models. This approach leads to a diverse ensemble that adapts well to different patterns in the data. In credit scoring, where different factors contribute to an individual's creditworthiness, having a diverse ensemble can be beneficial.\n",
    "\n",
    "Higher Predictive Accuracy:\n",
    "\n",
    "Boosting algorithms, such as Gradient Boosting, are known for their high predictive accuracy. In credit scoring, accurately predicting the likelihood of default is critical for making informed lending decisions. Boosting's ability to create highly accurate models can contribute to better risk assessment.\n",
    "In this example, boosting might be preferred for credit scoring due to its ability to handle imbalanced data, focus on difficult cases, and sequentially improve its predictions. The emphasis on correcting errors in a sequential manner aligns well with the goal of accurately assessing the credit risk of loan applicants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
